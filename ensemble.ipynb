{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code has three parts:\n",
    "Part A: coarse model training, all models trained on the large-scale Chexpert lung segmented image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from keras.models import Sequential, Model, Input, load_model\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, SeparableConv2D, BatchNormalization, ZeroPadding2D, GlobalAveragePooling2D,Flatten,Average, Dropout\n",
    "import time\n",
    "import statistics\n",
    "from keras import applications\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc, accuracy_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import scikitplot as skplt\n",
    "from itertools import cycle\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import load_model, Model, Sequential, Input\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important note: The size of the batch size plays an important role. \n",
    "#check whether the number of train, validation and test samples are\n",
    "#absolutely divisible by the batch size. if not, make sure to add 1\n",
    "# (+1) while fitting, evaluting and testing like this: do not use workers=1\n",
    "#reset the generators everytime before using them otherwise you will\n",
    "#get wierd results\n",
    "#example:\n",
    "#train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "#validation_steps=nb_validation_samples // batch_size + 1, verbose=1 \n",
    "#scorecustom = custom_model.evaluate_generator(validation_generator, nb_validation_samples // batch_size + 1, verbose = 1)\n",
    "#custom_y_pred = custom_model.predict_generator(test_generator, nb_test_samples//batch_size + 1, verbose=1)\n",
    "\n",
    "#%% Loading the training data\n",
    "#image dimensions and loading\n",
    "img_width, img_height = 256, 256\n",
    "train_data_dir = 'C:/Users/rajaramans2/codes/omsakthi_ensemble_visualization_kaggle/chexpert/data_binary_256/train/'\n",
    "test_data_dir = 'C:/Users/rajaramans2/codes/omsakthi_ensemble_visualization_kaggle/chexpert/data_binary_256/test/'\n",
    "epochs = 30\n",
    "batch_size = 8 \n",
    "num_classes= 2\n",
    "\n",
    "# Since the models work with the data of the same shape, we \n",
    "#define a single input layer that will be used by every model.\n",
    "\n",
    "input_shape = (img_width, img_height, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom confusion matrix function\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False, #if true all values in confusion matrix is between 0 and 1\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%declaring image data generators, make sure to delcare shuffle=False\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.1) #90/10, no augmentation except rescaling\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size, class_mode='categorical', subset = 'training')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size, class_mode='categorical', subset = 'validation')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',shuffle=False)\n",
    "\n",
    "#identify the number of samples\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(validation_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% assign class weights to balance model training and penalize over-represented classes\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% define custom model architecture \n",
    "\n",
    "def custom_cnn(model_input):\n",
    "    x = SeparableConv2D(64, (5, 5), padding='same', activation='relu')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(x)\n",
    "    \n",
    "    x = SeparableConv2D(128, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(x)\n",
    "    \n",
    "    x = SeparableConv2D(256, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2,2))(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=model_input, outputs=x, name='custom_cnn')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "custom_model = custom_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "custom_model.summary()\n",
    "\n",
    "#plot the model\n",
    "plot_model(custom_model, to_file='custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% VGG16 model \n",
    "\n",
    "def vgg16_cnn(model_input):\n",
    "    vgg16_cnn = VGG16(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = vgg16_cnn.output\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_vgg16')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=vgg16_cnn.input, outputs=predictions, name='vgg16_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "vgg16_custom_model = vgg16_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "vgg16_custom_model.summary()\n",
    "\n",
    "#plot the model\n",
    "plot_model(vgg16_custom_model, to_file='vgg16_custom_model.png',show_shapes=True, show_layer_names=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% VGG19 model \n",
    "\n",
    "def vgg19_cnn(model_input):\n",
    "    vgg19_cnn = VGG19(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = vgg19_cnn.output\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_vgg19')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=vgg19_cnn.input, outputs=predictions, name='vgg19_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "vgg19_custom_model = vgg19_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "vgg19_custom_model.summary()\n",
    "\n",
    "#plot the model\n",
    "plot_model(vgg19_custom_model, to_file='vgg19_custom_model.png',show_shapes=True, show_layer_names=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Inception-V3 model\n",
    "\n",
    "def inceptionv3_cnn(model_input):\n",
    "    inceptionv3_cnn = InceptionV3(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = inceptionv3_cnn.output\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_inceptionv3')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inceptionv3_cnn.input, outputs=predictions, name='inceptionv3_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "inceptionv3_custom_model = inceptionv3_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "inceptionv3_custom_model.summary()      \n",
    "\n",
    "#plot model\n",
    "plot_model(inceptionv3_custom_model, to_file='inceptionv3_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Xception model \n",
    "\n",
    "def xception_cnn(model_input):\n",
    "    xception_cnn = Xception(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = xception_cnn.output\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_xception')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=xception_cnn.input, outputs=predictions, name='xception_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "xception_custom_model = xception_cnn(model_input)\n",
    "\n",
    "#plot model summary\n",
    "xception_custom_model.summary()\n",
    "plot_model(xception_custom_model, to_file='xception_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% DenseNet121 model\n",
    "\n",
    "def densenet_cnn(model_input):\n",
    "    densenet_cnn = DenseNet121(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = densenet_cnn.output\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_densenet')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=densenet_cnn.input, outputs=predictions, name='densenet121_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "densenet_custom_model = densenet_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "densenet_custom_model.summary()\n",
    "\n",
    "#plot model\n",
    "plot_model(densenet_custom_model, to_file='densenet_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MobileNet model\n",
    "\n",
    "def mobile_cnn(model_input):\n",
    "    mobile_cnn = applications.mobilenet.MobileNet(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "    x = mobile_cnn.output\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_mobilenet')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=mobile_cnn.input, outputs=predictions, name='mobile_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "mobile_custom_model = mobile_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "mobile_custom_model.summary()\n",
    "\n",
    "#plot model\n",
    "plot_model(mobile_custom_model, to_file='mobile_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% NasNet Mobile model\n",
    "\n",
    "def NASNET_cnn(model_input):\n",
    "    NASNET_cnn = applications.nasnet.NASNetMobile(weights='imagenet', \n",
    "                                                    include_top=False, input_tensor=model_input)\n",
    "    x = NASNET_cnn.output\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_NASNET')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=NASNET_cnn.input, outputs=predictions, name='NASNET_custom')\n",
    "    return model\n",
    "\n",
    "#instantiate the model\n",
    "nasnet_custom_model = NASNET_cnn(model_input)\n",
    "\n",
    "#display model summary\n",
    "nasnet_custom_model.summary() \n",
    "\n",
    "#plot model\n",
    "plot_model(nasnet_custom_model, to_file='nasnet_custom_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile and train the custom model on the Chexpert data. Check if the train and\n",
    "#validation data are absolutely divisible by batch size.\n",
    "#Repeat for other pretrained models as well.\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "\n",
    "filepath = 'weights/' + custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"custom_coarse_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the VGG16 custom model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.95, nesterov=True)  \n",
    "vgg16_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + vgg16_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = vgg16_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"vgg16_custom_coarse_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the VGG19 custom model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.95, nesterov=True)  \n",
    "vgg19_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + vgg19_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = vgg19_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"vgg19_custom_coarse_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the InceptionV3 custom model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.95, nesterov=True)  \n",
    "inceptionv3_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "filepath = 'weights/' + inceptionv3_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = inceptionv3_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"InceptionV3_custom_coarse_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the Xception custom model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.95, nesterov=True)  \n",
    "xception_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + xception_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = xception_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"Xception_custom_coarse_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the DenseNet121 custom model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.95, nesterov=True)  \n",
    "densenet_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + densenet_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = densenet_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"DenseNet_custom_coarse_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the Mobilenet custom model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.95, nesterov=True)  \n",
    "mobile_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + mobile_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = mobile_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"Mobilenet_custom_coarse_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile and train the NASNET custom model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.95, nesterov=True)\n",
    "nasnet_custom_model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy']) \n",
    "\n",
    "filepath = 'weights/' + nasnet_custom_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = nasnet_custom_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"nasnet_custom_coarse_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation: Evaluate the CheXpert Trained models on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the models by loading the best weights\n",
    "custom_model.load_weights('weights/custom_cnn.06-0.8891.h5')\n",
    "custom_model.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "custom_y_pred = custom_model.predict_generator(test_generator,\n",
    "                                        nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "import itertools\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the VGG16 model by loading the best weights\n",
    "vgg16_custom_model.load_weights('weights/vgg16_custom.10-0.9209.h5')\n",
    "vgg16_custom_model.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "vgg16_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "vgg16_custom_y_pred = vgg16_custom_model.predict_generator(test_generator,\n",
    "                                        nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "import itertools\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,vgg16_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the VGG19 model by loading the best weights\n",
    "vgg19_custom_model.load_weights('weights/vgg19_custom.05-0.9197.h5')\n",
    "vgg19_custom_model.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "vgg19_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "vgg19_custom_y_pred = vgg19_custom_model.predict_generator(test_generator,\n",
    "                                        nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "import itertools\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,vgg19_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Inception-V3 model by loading the best weights\n",
    "inceptionv3_custom_model.load_weights('weights/inceptionv3_custom.10-0.9179.h5')\n",
    "inceptionv3_custom_model.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "inceptionv3_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "inceptionv3_custom_y_pred = inceptionv3_custom_model.predict_generator(test_generator,\n",
    "                                        nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "import itertools\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,inceptionv3_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Xception model by loading the best weights\n",
    "xception_custom_model.load_weights('weights/xception_custom.02-0.9167.h5')\n",
    "xception_custom_model.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "xception_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "xception_custom_y_pred = xception_custom_model.predict_generator(test_generator,\n",
    "                                        nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "import itertools\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,xception_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Densenet-121 model by loading the best weights\n",
    "densenet_custom_model.load_weights('weights/densenet121_custom.01-0.9155.h5')\n",
    "densenet_custom_model.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "densenet_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "densenet_custom_y_pred = densenet_custom_model.predict_generator(test_generator,\n",
    "                                        nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "import itertools\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,densenet_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the MobileNet model by loading the best weights\n",
    "mobile_custom_model.load_weights('weights/mobile_custom.03-0.9172.h5')\n",
    "mobile_custom_model.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "mobile_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "mobile_custom_y_pred = mobile_custom_model.predict_generator(test_generator,\n",
    "                                        nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "import itertools\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,mobile_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the NasNet Mobile model by loading the best weights\n",
    "nasnet_custom_model.load_weights('weights/NASNET_custom.05-0.9178.h5')\n",
    "nasnet_custom_model.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "nasnet_custom_model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "nasnet_custom_y_pred = nasnet_custom_model.predict_generator(test_generator,\n",
    "                                        nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "import itertools\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=False, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,nasnet_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B:\n",
    "\n",
    "By this time, all the custom and pretrained models are entirely retrained on the large-scale ChexPert data. The best model are stored. The models are reloaded and truncated at the intermediate layers (determined empirically) that gave the best performance on the Kaggle Pneumonia dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the Kaggle abnormality classifier data\n",
    "\n",
    "img_width, img_height = 256, 256\n",
    "num_classes = 2\n",
    "train_data_dir = 'C:/abnormality_classifier_binary/abnormality_aug/train'\n",
    "test_data_dir = 'C:/abnormality_classifier_binary/abnormality_aug/test'\n",
    "epochs = 30\n",
    "batch_size = 8 \n",
    "\n",
    "# Since the models work with the data of the same shape, we \n",
    "#define a single input layer that will be used by every model.\n",
    "\n",
    "input_shape = (img_width, img_height, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%declaring image data generators, make sure to delcare shuffle=False\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.1) #90/10, no augmentation except rescaling\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size, class_mode='categorical', subset = 'training')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size, class_mode='categorical', subset = 'validation')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',shuffle=False)\n",
    "\n",
    "#identify the number of samples\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(validation_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% assign class weights to balance model training and penalize over-represented classes\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               'balanced',\n",
    "                np.unique(train_generator.classes), \n",
    "                train_generator.classes)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.load_weights('weights/custom_cnn.06-0.8891.h5')\n",
    "custom_model.summary()\n",
    "base_model_custom=Model(inputs=custom_model.input,outputs=custom_model.get_layer('separable_conv2d_3').output)\n",
    "\n",
    "#addind the top layers\n",
    "x = base_model_custom.output\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_custom')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model_custom = Model(inputs=base_model_custom.input, outputs=predictions, name = 'custom_finetuned')\n",
    "model_custom.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_custom.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + model_custom.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = model_custom.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"weights/custom_coarse_to_fine.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the models by loading the best weights\n",
    "model_custom.load_weights('weights/custom_finetuned.04-0.8442.h5')\n",
    "model_custom.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_custom.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "custom_y_pred = model_custom.predict_generator(test_generator, \n",
    "                                                           nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16 model\n",
    "vgg16_custom_model.load_weights('weights/vgg16_custom.10-0.9209.h5')\n",
    "vgg16_custom_model.summary()\n",
    "base_model_vgg16=Model(inputs=vgg16_custom_model.input,outputs=vgg16_custom_model.get_layer('block5_conv3').output)\n",
    "#addind the top layers\n",
    "x = base_model_vgg16.output\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_vgg16')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model_vgg16 = Model(inputs=base_model_vgg16.input, outputs=predictions, name = 'vgg16_finetuned')\n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_vgg16.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + model_vgg16.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = model_vgg16.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"weights/vgg16_coarse_to_fine.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the models by loading the best weights\n",
    "model_vgg16.load_weights('weights/vgg16_finetuned.06-0.8946.h5')\n",
    "model_vgg16.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_vgg16.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "vgg16_custom_y_pred = model_vgg16.predict_generator(test_generator, \n",
    "                                                           nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),vgg16_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,vgg16_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG19 model\n",
    "vgg19_custom_model.load_weights('weights/vgg19_custom.05-0.9197.h5')\n",
    "vgg19_custom_model.summary()\n",
    "base_model_vgg19=Model(inputs=vgg19_custom_model.input,outputs=vgg19_custom_model.get_layer('block4_pool').output)\n",
    "#addind the top layers\n",
    "x = base_model_vgg16.output\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_vgg19')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model_vgg19 = Model(inputs=base_model_vgg19.input, outputs=predictions, name = 'vgg19_finetuned')\n",
    "model_vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_vgg19.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + model_vgg19.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = model_vgg19.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"weights/vgg19_coarse_to_fine.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the models by loading the best weights\n",
    "model_vgg19.load_weights('weights/vgg19_finetuned.02-0.8921.h5')\n",
    "model_vgg19.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_vgg19.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "vgg19_custom_y_pred = model_vgg19.predict_generator(test_generator, \n",
    "                                                           nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),vgg19_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,vgg19_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inception-v3 model\n",
    "inceptionv3_custom_model.load_weights('weights/inceptionv3_custom.10-0.9179.h5')\n",
    "inceptionv3_custom_model.summary()\n",
    "base_model_inceptionv3=Model(inputs=inceptionv3_custom_model.input,\n",
    "                             outputs=inceptionv3_custom_model.get_layer('mixed3').output)\n",
    "#addind the top layers\n",
    "x = base_model_inceptionv3.output\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_inceptionv3')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model_inceptionv3 = Model(inputs=base_model_inceptionv3.input, outputs=predictions, name = 'inceptionv3_finetuned')\n",
    "model_inceptionv3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_inceptionv3.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + model_inceptionv3.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = model_inceptionv3.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"weights/inceptionv3_coarse_to_fine.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Inception-V3 model by loading the best weights\n",
    "model_inceptionv3.load_weights('weights/inceptionv3_finetuned.03-0.8821.h5')\n",
    "model_inceptionv3.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_inceptionv3.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "inceptionv3_custom_y_pred = model_inceptionv3.predict_generator(test_generator, \n",
    "                                                           nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),inceptionv3_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,inceptionv3_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xception model\n",
    "xception_custom_model.load_weights('weights/xception_custom.02-0.9167.h5')\n",
    "xception_custom_model.summary()\n",
    "base_model_xception=Model(inputs=xception_custom_model.input,\n",
    "                             outputs=xception_custom_model.get_layer('add_7').output)\n",
    "#addind the top layers\n",
    "x = base_model_xception.output\n",
    "x = Activation('relu') (x)\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_xception')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model_xception = Model(inputs=base_model_xception.input, outputs=predictions, name = 'xception_finetuned')\n",
    "model_xception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_xception.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + model_xception.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = model_xception.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"weights/xception_coarse_to_fine.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Inception-V3 model by loading the best weights\n",
    "model_xception.load_weights('weights/xception_finetuned.08-0.8791.h5')\n",
    "model_xception.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_xception.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "xception_custom_y_pred = model_xception.predict_generator(test_generator, \n",
    "                                                           nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),xception_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,xception_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best model weights: DenseNet121\n",
    "densenet_custom_model.load_weights('weights/densenet121_custom.01-0.9155.h5'')\n",
    "densenet_custom_model.summary()\n",
    "base_model_densenet=Model(inputs=densenet_custom_model.input,outputs=densenet_custom_model.get_layer('pool3_conv').output)\n",
    "\n",
    "\n",
    "#addind the top layers\n",
    "x = base_model_densenet.output\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_densenet')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model_densenet = Model(inputs=base_model_densenet.input, outputs=predictions, name = 'densenet_finetuned')\n",
    "model_densenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_densenet.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + model_densenet.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = model_densenet.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"weights/densenet_coarse_to_fine.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the models by loading the best weights\n",
    "model_densenet.load_weights('weights/densenet_finetuned.06-0.8873.h5')\n",
    "model_densenet.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_densenet.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "densenet_custom_y_pred = model_densenet.predict_generator(test_generator, \n",
    "                                                           nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance metrics\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),densenet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,densenet_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best model weights: MobileNet\n",
    "mobile_custom_model.load_weights('weights/mobile_custom.03-0.9172.h5')\n",
    "mobile_custom_model.summary()\n",
    "base_model_mobile=Model(inputs=mobile_custom_model.input,outputs=mobile_custom_model.get_layer('conv_pw_6_relu').output)\n",
    "#addind the top layers\n",
    "x = base_model_mobile.output\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_mobile')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model_mobile = Model(inputs=base_model_mobile.input, outputs=predictions, name = 'mobilenet_finetuned')\n",
    "model_mobile.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_mobile.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + model_mobile.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = model_mobile.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"weights/mobilenet_coarse_to_fine.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the models by loading the best weights: MobileNet\n",
    "model_mobile.load_weights('weights/mobilenet_finetuned.07-0.8801.h5')\n",
    "model_mobile.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_mobile.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "mobile_custom_y_pred = model_mobile.predict_generator(test_generator, \n",
    "                                                           nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),mobile_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,mobile_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best model weights: NasNet Mobile\n",
    "nasnet_custom_model.load_weights('weights/NASNET_custom.05-0.9178.h5')\n",
    "nasnet_custom_model.summary()\n",
    "base_model_nasnet=Model(inputs=nasnet_custom_model.input,outputs=nasnet_custom_model.get_layer('activation_129').output)\n",
    "#addind the top layers\n",
    "x = base_model_nasnet.output\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(1024, (3, 3), activation='relu', name='extra_conv_nasnet')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "model_nasnet = Model(inputs=base_model_nasnet.input, outputs=predictions, name = 'nasnet_finetuned')\n",
    "model_nasnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compile and train the model\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_nasnet.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "filepath = 'weights/' + model_nasnet.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "history = model_nasnet.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=300)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"weights/nasnet_coarse_to_fine.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the models by loading the best weights\n",
    "model_nasnet.load_weights('weights/nasnet_finetuned.04-0.8740.h5')\n",
    "model_nasnet.summary()\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_nasnet.compile(optimizer=sgd,loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "#measure performance on test data, first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "nasnet_custom_y_pred = model_nasnet.predict_generator(test_generator, \n",
    "                                                           nb_test_samples // batch_size + 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot normalized confusion matrix using scikit plot\n",
    "skplt.metrics.plot_confusion_matrix(Y_test1.argmax(axis=-1),nasnet_custom_y_pred.argmax(axis=-1),\n",
    "                                    normalize=True, x_tick_rotation=45, figsize=(20,10),\n",
    "                                    title_fontsize='large', text_fontsize='medium')\n",
    "plt.show()\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=300)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "skplt.metrics.plot_roc(Y_test,nasnet_custom_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part - C: Now that the models are finetuned on the kaggle pneumonia data and the best models are stored, we further take benefit of the Ensembles to improve the performance than any individual constituent model.The best models trained on the abnormality data are ensembled using several ensemble learning strategies including majority voting, simple averaging, weighted averaging and stacked generalization. We kept the sequential CNN as the baseline and didnt use them in ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets do a dummy assignment of the predictions\n",
    "\n",
    "vgg16_custom_y_pred1 = vgg16_custom_y_pred\n",
    "vgg19_custom_y_pred1 = vgg19_custom_y_pred\n",
    "xception_custom_y_pred1 = xception_custom_y_pred\n",
    "inceptionv3_custom_y_pred1 = inceptionv3_custom_y_pred\n",
    "densenet_custom_y_pred1 = densenet_custom_y_pred\n",
    "nasnet_custom_y_pred1 = nasnet_custom_y_pred\n",
    "mobile_custom_y_pred1 = mobile_custom_y_pred\n",
    "\n",
    "#print the shape of the predictions\n",
    "print(\"The shape of VGG16 custom model prediction vgg16_custom_y_pred is  = \", vgg16_custom_y_pred1.shape)\n",
    "print(\"The shape of VGG19 custom model prediction vgg19_custom_y_pred is  = \", vgg19_custom_y_pred1.shape)\n",
    "print(\"The shape of inceptionv3 custom model prediction inceptionv3_custom_y_pred is = \", inceptionv3_custom_y_pred1.shape)\n",
    "print(\"The shape of densenet custom model prediction densenet_custom_y_pred is  = \", densenet_custom_y_pred1.shape)\n",
    "print(\"The shape of Xception custom model prediction xception_custom_y_pred is = \", xception_custom_y_pred1.shape)\n",
    "print(\"The shape of NASNET custom model prediction nasnet_custom_y_pred is  = \", nasnet_custom_y_pred1.shape)\n",
    "print(\"The shape of MobileNet custom model prediction mobile_custom_y_pred is  = \", mobile_custom_y_pred1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_custom_y_pred1 = vgg16_custom_y_pred1.argmax(axis=-1)\n",
    "print(vgg16_custom_y_pred1)\n",
    "vgg19_custom_y_pred1 = vgg19_custom_y_pred1.argmax(axis=-1)\n",
    "print(vgg19_custom_y_pred1)\n",
    "inceptionv3_custom_y_pred1 = inceptionv3_custom_y_pred1.argmax(axis=-1)\n",
    "print(inceptionv3_custom_y_pred1)\n",
    "densenet_custom_y_pred1 = densenet_custom_y_pred1.argmax(axis=-1)\n",
    "print(densenet_custom_y_pred1)\n",
    "xception_custom_y_pred1 = xception_custom_y_pred1.argmax(axis=-1)\n",
    "print(xception_custom_y_pred1)\n",
    "nasnet_custom_y_pred1 = nasnet_custom_y_pred1.argmax(axis=-1)\n",
    "print(nasnet_custom_y_pred1)\n",
    "mobile_custom_y_pred1 = mobile_custom_y_pred1.argmax(axis=-1)\n",
    "print(mobile_custom_y_pred1)\n",
    "\n",
    "#max voting begins\n",
    "max_voting_pred = np.array([])\n",
    "for i in range(0,len(test_generator.filenames)):\n",
    "    max_voting_pred = np.append(max_voting_pred, \n",
    "                                statistics.mode([vgg16_custom_y_pred1[i], vgg19_custom_y_pred1[i],\n",
    "                                                 inceptionv3_custom_y_pred1[i], densenet_custom_y_pred1[i],\n",
    "                                                 xception_custom_y_pred1[i],\n",
    "                                                 nasnet_custom_y_pred1[i], mobile_custom_y_pred1[i],\n",
    "                                                ]))\n",
    "    \n",
    "ensemble_model_max_voting_accuracy = accuracy_score(Y_test,max_voting_pred)\n",
    "print(\"The max voting accuracy of the ensemble model is  = \", ensemble_model_max_voting_accuracy)\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('weights/max_voting_y_pred.csv',max_voting_pred,fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] \n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test,max_voting_pred,target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,max_voting_pred)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for Max Voting ensemble without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate error\n",
    "\n",
    "ensemble_model_maxvoting_mean_squared_error = mean_squared_error(Y_test,max_voting_pred)  \n",
    "ensemble_model_maxvoting_mean_squared_log_error = mean_squared_log_error(Y_test,max_voting_pred)  \n",
    "print(\"The max voting mean squared error of the ensemble model is  = \", ensemble_model_maxvoting_mean_squared_error)\n",
    "print(\"The max voting mean squared log error of the ensemble model is  = \", ensemble_model_maxvoting_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE AVERAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets perform simple averaging of the predictions from individual models\n",
    "\n",
    "average_pred=(inceptionv3_custom_y_pred + vgg19_custom_y_pred +\n",
    "              densenet_custom_y_pred + xception_custom_y_pred +\n",
    "              vgg16_custom_y_pred + nasnet_custom_y_pred +\n",
    "              mobile_custom_y_pred)/7\n",
    "\n",
    "#compute simple averaging accuracy\n",
    "ensemble_model_averaging_accuracy = accuracy_score(Y_test,average_pred.argmax(axis=-1))\n",
    "print(\"The averaging accuracy of the ensemble model is  = \", ensemble_model_averaging_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] \n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test,average_pred.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,average_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for Average Ensemble without normalization')\n",
    "plt.show()\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('weights/averaging_y_pred.csv',average_pred.argmax(axis=-1),fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curves\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,average_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        average_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], average_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   average_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, average_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate simple averaging error\n",
    "\n",
    "ensemble_model_averaging_mean_squared_error = mean_squared_error(Y_test,average_pred.argmax(axis=-1))  \n",
    "ensemble_model_averaging_mean_squared_log_error = mean_squared_log_error(Y_test,average_pred.argmax(axis=-1))  \n",
    "print(\"The averaging mean squared error of the ensemble model is  = \", ensemble_model_averaging_mean_squared_error)\n",
    "print(\"The averaging mean squared log error of the ensemble model is  = \", ensemble_model_averaging_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning equal weights for top-2 models and equal weights for the other five models gave best results\n",
    "#in this task, VGG16 got the best results, followed by VGG19. \n",
    "# so giving higher weightage to vgg16 and vgg19 (0.25) and equal weightages (0.1) to other models.\n",
    "\n",
    "weighted_average_pred=(vgg16_custom_y_pred * 0.25 + vgg19_custom_y_pred * 0.25 +\n",
    "                       inceptionv3_custom_y_pred * 0.1 + densenet_custom_y_pred * 0.1 + \n",
    "                       xception_custom_y_pred * 0.1 + nasnet_custom_y_pred * 0.1 +\n",
    "                       mobile_custom_y_pred * 0.1)\n",
    "\n",
    "#calculate weighted averaging accuracy\n",
    "ensemble_model_weighted_averaging_accuracy = accuracy_score(Y_test,weighted_average_pred.argmax(axis=-1))\n",
    "print(\"The weighted averaging accuracy of the ensemble model is  = \", ensemble_model_weighted_averaging_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)', 'class 1(normal)'] #modify according to tasks\n",
    "\n",
    "#print classification report\n",
    "print(classification_report(Y_test,weighted_average_pred.argmax(axis=-1),target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test,weighted_average_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix for Weighted Average Ensemble without normalization')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('weights/weighted_averaging_y_pred.csv',weighted_average_pred.argmax(axis=-1),fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot roc curves\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,weighted_average_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        weighted_average_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], weighted_average_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   weighted_average_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, weighted_average_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate error\n",
    "ensemble_model_weighted_average_mean_squared_error = mean_squared_error(Y_test,\n",
    "                                                                        weighted_average_pred.argmax(axis=-1))  \n",
    "ensemble_model_weighted_average_mean_squared_log_error = mean_squared_log_error(Y_test,\n",
    "                                                                                weighted_average_pred.argmax(axis=-1))  \n",
    "print(\"The weighted averaging mean squared error of the ensemble model is  = \", \n",
    "      ensemble_model_weighted_average_mean_squared_error)\n",
    "print(\"The weighted averaging mean squared log error of the ensemble model is  = \", \n",
    "      ensemble_model_weighted_average_mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted performing a stacking ensemble by training a neural network based meta-learner that will best combine the predictions from the sub-models and ideally perform better than any single sub-model.The first step is to load the saved models. We can use the load_model() Keras function and create a Python list of loaded models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "n_models = 7 #we have seven models\n",
    "\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    model_densenet.load_weights('weights/densenet_finetuned.06-0.8873.h5')\n",
    "    all_models.append(model_densenet)\n",
    "    model_inceptionv3.load_weights('weights/inceptionv3_finetuned.03-0.8821.h5')\n",
    "    all_models.append(model_inceptionv3)\n",
    "    model_vgg19.load_weights('weights/vgg19_finetuned.02-0.8921.h5')\n",
    "    all_models.append(model_vgg19)\n",
    "    model_vgg16.load_weights('weights/vgg16_finetuned.06-0.8946.h5')\n",
    "    all_models.append(model_vgg16)\n",
    "    model_xception.load_weights('weights/xception_finetuned.08-0.8791.h5')\n",
    "    all_models.append(model_xception)\n",
    "    model_nasnet.load_weights('weights/nasnet_finetuned.04-0.8740.h5')\n",
    "    all_models.append(model_nasnet)\n",
    "    model_mobile.load_weights('weights/mobilenet_finetuned.07-0.8801.h5')\n",
    "    all_models.append(model_mobile)\n",
    "    return all_models\n",
    "\n",
    "# We can call this function to load our three saved models from the “models/” sub-directory.\n",
    "# load all models\n",
    "\n",
    "n_members = 7\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to know how well the single models perform on the test dataset as we would expect a stacking model to perform better. We can easily evaluate each single model on the training dataset and establish a baseline of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate standalone models on test dataset\n",
    "\n",
    "for model in members:\n",
    "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) \n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    _, acc = model.evaluate_generator(test_generator, nb_test_samples//batch_size + 1, verbose=1)\n",
    "    print('Model Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Stacking Model: It may be desirable to use a neural network as a meta-learner.Specifically, the sub-networks can be embedded in a larger multi-headed neural network that then learns how to best combine the predictions from each input sub-model. It allows the stacking ensemble to be treated as a single large model. The benefit of this approach is that the outputs of the submodels are provided directly to the meta-learner. Further, it is also possible to update the weights of the submodels in conjunction with the meta-learner model, if this is desirable. This can be achieved using the Keras functional interface for developing models. \n",
    "\n",
    "After the models are loaded as a list, a larger stacking ensemble model can be defined where each of the loaded models is used as a separate input-head to the model. All of the layers in each of the loaded models be marked as not trainable so the weights cannot be updated when the new larger model is being trained. Keras also requires that each layer has a unique name, therefore the names of each layer in each of the loaded models will have to be updated to indicate to which ensemble member they belong. Once the sub-models have been prepared, we can define the stacking ensemble model. The input layer for each of the sub-models will be used as a separate input head to this new model. This means that k copies of any input data will have to be provided to the model, where k is the number of input models, in this case, 3. The outputs of each of the models can then be merged. In this case, we will use a simple concatenation merge, where a single 6-element vector will be created from the two class-probabilities predicted by each of the 3 models. \n",
    "\n",
    "We will then define a hidden layer to interpret this “input” to the meta-learner and an output layer that will make its own probabilistic prediction. A plot of the network graph is created when this function is called to give an idea of how the ensemble model fits together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "        # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer.name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(14, activation='relu')(merge) # two ouputs for 7 models, so 14 hidden neurons\n",
    "    output = Dense(2, activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    \n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "stacked_model.summary()\n",
    "\n",
    "#plot model\n",
    "plot_model(stacked_model, to_file='stacked_model.png',show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is defined, it can be fit. We can fit it directly on the holdout validation dataset. Because the sub-models are not trainable, their weights will not be updated during training and only the weights of the new hidden and output layer will be updated. The stacking neural network model will be fit on the trainig data for 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the ensemble model\n",
    "filepath = 'weights/' + stacked_model.name + '.{epoch:02d}-{val_acc:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_weights_only=False, save_best_only=True, mode='max', period=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5,\n",
    "                              verbose=1, mode='max', min_lr=0.00001)\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='logs/', histogram_freq=0, batch_size=batch_size)\n",
    "callbacks_list = [checkpoint, tensor_board, reduce_lr]\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "history = stacked_model.fit_generator(train_generator, steps_per_epoch=nb_train_samples // batch_size + 1,\n",
    "                                  epochs=30, validation_data=validation_generator,\n",
    "                                  class_weight = class_weights,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size + 1, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot performance of the ensemble model\n",
    "\n",
    "N = 30\n",
    "plt.figure(figsize=(20,10), dpi=100)\n",
    "plt.plot(np.arange(1, N+1), history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"acc\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), history.history[\"val_acc\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"weights/stacking_ensemble_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once fit, we can use the new stacked model to make a prediction on new data.\n",
    "# This is as simple as calling the predict_generator() function on the model. \n",
    "\n",
    "#load the best model\n",
    "stacked_model.load_weights('weights/stacking_ensemble.03-0.8907.h5')\n",
    "stacked_model.summary()\n",
    "\n",
    "#first reset the test generator otherwise it gives wierd results\n",
    "test_generator.reset()\n",
    "\n",
    "#evaluate accuracy \n",
    "ensemble_y_pred = stacked_model.predict_generator(test_generator, nb_test_samples//batch_size + 1, verbose=1)\n",
    "\n",
    "#print prediction shapes\n",
    "print(ensemble_y_pred.shape)\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('weights/stacking_y_pred.csv', ensemble_y_pred, fmt='%i',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure performance metrics of the stacked ensemble\n",
    "\n",
    "accuracy = accuracy_score(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))\n",
    "print('The test accuracy of the Custom model is: ', accuracy)\n",
    "\n",
    "#evaluate mean squared error\n",
    "custom_mse = mean_squared_error(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))\n",
    "print('The Mean Squared Error of the Custom model is: ', custom_mse)\n",
    "\n",
    "#evaluate mean squared log error\n",
    "custom_msle = mean_squared_log_error(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))  \n",
    "print('The Mean Squared Log Error of the Custom model is: ', custom_msle)\n",
    "\n",
    "#evaluate matthews correlation coefficient\n",
    "custom_MCC = matthews_corrcoef(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))\n",
    "print('The Matthews correlation coefficient value (MCC) for the Custom model is: ', custom_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% print classification report and plot confusion matrix\n",
    "\n",
    "target_names = ['class 0(abnormal)','class 1(normal)'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),ensemble_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Plot non-normalized confusion matrix using scikit learn\n",
    "plt.figure(figsize=(10,10), dpi=100)\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compute the ROC-AUC values\n",
    "\n",
    "skplt.metrics.plot_roc(Y_test,ensemble_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precision-recall curves\n",
    "\n",
    "colors = cycle(['red', 'blue', 'green', 'cyan', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(15,10), dpi=100)\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(num_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test1[:, i],\n",
    "                                                        ensemble_y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test1[:, i], ensemble_y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test1.ravel(),\n",
    "   ensemble_y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test1, ensemble_y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.4f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.4f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.05)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the KS statistic plot\n",
    "\n",
    "skplt.metrics.plot_ks_statistic(Y_test,ensemble_y_pred,figsize=(20,10),\n",
    "                       title_fontsize='large', text_fontsize='large')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model has its own weaknesses. The reasoning behind using an ensemble is that by stacking different models representing different hypotheses about the data, we can find a better hypothesis that is not in the hypothesis space of the models from which the ensemble is built. By using a very basic ensemble, a much lower error rate was achieved than when a single model was used. This proves effectiveness of ensembling. Of course, there are some practical considerations to keep in mind when using an ensemble for your machine learning task. Since ensembling means stacking multiple models together, it also means that the input data needs to be forward-propagated for each model. This increases the amount of compute that needs to be performed and, consequently, evaluation (predicition) time. However, it is a very critical factor when designing a commercial product. Another consideration is increased size of the final model which, again, might be a limiting factor for ensemble use in a commercial product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
